import numpy as np
from scipy.spatial.distance import cosine
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("semantic-search-server")

# Mock vector database for demonstration
class VectorDB:
    def __init__(self):
        # In a real implementation, this would load from a database
        self.documents = [
            {"id": 1, "title": "Introduction to Python", "content": "Python is a programming language..."},
            {"id": 2, "title": "Machine Learning Basics", "content": "Machine learning is a subset of AI..."},
            {"id": 3, "title": "Data Visualization", "content": "Visualizing data helps in understanding patterns..."}
        ]
        
        # Mock embeddings (in practice, these would be generated by an embedding model)
        self.embeddings = {
            1: np.array([0.1, 0.2, 0.3]),
            2: np.array([0.2, 0.3, 0.4]),
            3: np.array([0.3, 0.4, 0.5])
        }
    
    def search(self, query_embedding, top_k=2):
        # Calculate similarity scores
        scores = {}
        for doc_id, embedding in self.embeddings.items():
            scores[doc_id] = 1 - cosine(query_embedding, embedding)  # Cosine similarity
        
        # Sort by score
        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)[:top_k]
        
        # Return documents
        return [
            {**self.documents[doc_id-1], "score": scores[doc_id]}
            for doc_id in sorted_ids
        ]

# Initialize vector database
vector_db = VectorDB()

@mcp.tool()
async def semantic_search(query: str, top_k: int = 2) -> str:
    """Search documents semantically based on meaning, not just keywords.
    
    Args:
        query: Search query
        top_k: Number of top results to return
    """
    # In a real implementation, we would generate the query embedding here
    # For this example, we'll simulate it with a simple function
    def mock_get_embedding(text):
        # This is a placeholder; real embeddings would come from a model
        return np.array([0.2, 0.3, 0.4])
    
    query_embedding = mock_get_embedding(query)
    
    # Search the vector database
    results = vector_db.search(query_embedding, top_k=top_k)
    
    # Format results
    formatted_results = []
    for i, doc in enumerate(results):
        formatted_results.append(
            f"{i+1}. {doc['title']} (Score: {doc['score']:.2f})\n"
            f"   {doc['content'][:100]}..."
        )
    
    if not formatted_results:
        return "No matching documents found."
    
    return "Search results:\n\n" + "\n\n".join(formatted_results)
